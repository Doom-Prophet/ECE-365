{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Principal Component Analysis Final "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: zicheng5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: March 3, 2021 at 11:59 PM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the <a href=\"https://courses.engr.illinois.edu/ece398bd/\">course website</a>. This is the last lab for this section of the course. Make sure to be up to date for the policies of the second part of the course. **You will have another lab next week and a different TA (who is not familiar with this lab), so it is in your best interests to finish this lab before next week's lab session.**\n",
    "\n",
    "The submission procedure is provided below:\n",
    "- You will be provided with a template Python script (main.py) for this lab where you need to implement the provided functions as needed for each question. Follow the instructions provided in this Jupyter Notebook (.ipynb) to implement the required functions. **Do not change the file name or the function headers!**\n",
    "- Upload only your Python script (.py file) on Gradescope. Don't upload your datasets or Jupyter Notebook (.ipynb file).\n",
    "- Your grades and feedbacks will appear on Gradescope. The grading for the programming questions is automated using Gradescope autograder, no partial credits are given. Therefore, if you wish, you will have a chance to re-submit your code **within 72 hours** of receiving your first grade for this lab, only if you have *reasonable* submissions before the deadline (i.e. not an empty script).\n",
    "- If you re-submit, the final grade for the programming part of this lab will be calculated as .4 \\* first_grade + .6 \\* .9 \\* re-submission_grade.\n",
    "- This lab also has Multiple Choice Questions (MCQs) that are needed to be completed on Gradescope **within the deadline**.\n",
    "\n",
    "There are some problems which have short answer questions. They are not graded, but we are free to discuss answers to these problems. **Multiple Choice Questions (MCQs) will be graded on Gradescope!**\n",
    "\n",
    "Remember in many applications, the end goal is not always \"run a classifier\", like in a homework problem, but is to use the output of the classifier in the context of the problem at hand (e.g. detecting spam, identifying cancer, etc.). Because of this, some of our Engineering Design-type questions are designed to get you to think about the entire design problem at a high level.\n",
    "\n",
    "\n",
    "**Warning: Do not train on your test sets. You will automatically have your score halved for a problem if you train on your test data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Will Need To Know For This Lab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eigendecomposition\n",
    "- Singular Value Decomposition\n",
    "- Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble (Don't change this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.spatial.distance as dist\n",
    "from matplotlib.colors import ListedColormap\n",
    "%run main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_interactive=False # If you want to rotate plots, set this to True. \n",
    "# When submitting your notebook, enable_interactive=False and run the whole notebook. \n",
    "# The interactive stuff can be a bit glitchy, so if you're having trouble, turn them off. \n",
    "if enable_interactive:\n",
    "    # These packages allow us to rotate plots and what not.\n",
    "    from IPython.display import display\n",
    "    from IPython.html.widgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Visualizing Principal Components (45 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will be implementing PCA, visualizing the principal components and using it to perform dimensionality reduction. \n",
    "\n",
    "Do not use a pre-written implementation of PCA for this problem (e.g. sklearn.decomposition.PCA). You should assume that the input data has been appropriately pre-processed to have zero-mean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will generate some data.\n",
    "numpy.random.seed(seed=2232017)\n",
    "true_cov = np.array([[1,.5,.2],[.5,1,.3],[.2,.3,1]]) #This is the true covariance matrix \n",
    "                                                     #of the data. Do not use it in your code!\n",
    "data=(np.random.randn(1000,3)).dot(np.linalg.cholesky(true_cov).T) \n",
    "\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we visualize the data using a 3D scatterplot. \n",
    "\n",
    "Our data is stored in a variable called `data` where each row is a feature vector (with three features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(data[:,0],data[:,1],data[:,2])\n",
    "if enable_interactive:\n",
    "    @interact(elev=(-90, 90), azim=(0, 360))\n",
    "    def view(elev, azim):\n",
    "        ax.view_init(elev, azim)\n",
    "        display(ax.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `pcaeig` which implements PCA via the eigendecomposition. <b>(15 points)</b>\n",
    "\n",
    "You will be given as input:\n",
    "- A $(N,d)$ numpy array of data (with each row as a feature vector)\n",
    "\n",
    "Your function should return a tuple consisting of the PCA transformation matrix (which is $(d,d)$), and a vector consisting of the amount of variance explained in the data by each PCA feature. Note that each row of the $(d,d)$ matrix should contain a principal component. Also note that the PCA features are ordered in decreasing amount of variance explained, by convention.\n",
    "\n",
    "Hints:\n",
    "- The function <a href=\"http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.eigh.html\">numpy.linalg.eigh</a> will be useful. Note that it returns its eigenvalues in *ascending* order. `numpy.fliplr` or similar may be useful as well.\n",
    "- You can calculate the covariance matrix of the data by multiplying the data matrix with its transpose in the appropriate order, and scaling it. \n",
    "- Do not use numpy.cov -- we are assuming the data has zero mean beforehand, so the number of degrees of freedom is different (since the covariance estimate knows the mean in our case). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will run PCA on your data, store your PCA transformation in a variable called `W`, and the amount of variance explained by each PCA feature in a variable called `s`, and print out the principal components (i.e. the rows of `W`) along with the corresponding amount of variance explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, run PCA on your data. The PCA transformation is stored in W, while the amount of variance is stored in s. \n",
    "q1 = Question1()\n",
    "\n",
    "W,s = q1.pcaeig(data)\n",
    "\n",
    "# Print out the principal components + the amount of variance they explain\n",
    "for i in range(W.shape[1]):\n",
    "    print (i+1,\"-th principal component: \", W[i,:], \"\\t Variance:\",s[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the principal components on top of our data. The first principal component is in red, and captures the most variance. The second principal component is in green, while the last principal component is in yellow.\n",
    "\n",
    "We generated our data from am *elliptical distribution*, so it should be easy to visualize these components as the axes of the data (which looks like an ellipsoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figb = plt.figure()\n",
    "axb = Axes3D(figb)\n",
    "axb.scatter(data[:,0],data[:,1],data[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axb.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)\n",
    "if enable_interactive:\n",
    "    @interact(elev=(-90, 90), azim=(0, 360))\n",
    "    def view(elev, azim):\n",
    "        axb.view_init(elev, azim)\n",
    "        display(axb.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, the red line should be longer than the green line which should be longer than the yellow line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will implement functions to generate PCA features.\n",
    "\n",
    "Write a function `pcadimreduce` which implements dimension reduction via PCA. It takes in three inputs:\n",
    "- A $(N,d)$ numpy array, `data`, with each row as a feature vector\n",
    "- A $(d,d)$ numpy array, `W`, the PCA transformation matrix (e.g. generated from `pcaeig` or `pcasvd`)\n",
    "- A number `k`, which is the number of PCA features to retain\n",
    "\n",
    "It should return a $(N,k)$ numpy array, where the $i$-th row contains the PCA features corresponding to the $i$-th input feature vector. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `pcareconstruct` which reconstructs the original features from the PCA features. It takes in three inputs:\n",
    "- A $(N,k)$ numpy array, `pcadata`, with each row as a PCA feature vector (e.g. generated from `pcadimreduce`)\n",
    "- A $(d,d)$ numpy array, `W`, the PCA transformation matrix (e.g. generated from `pcaeig` or `pcasvd`)\n",
    "- A number `k`, which is the number of PCA features\n",
    "\n",
    "It should return a $(N,d)$ numpy array, where the $i$-th row contains the reconstruction of the original $i$-th input feature vector (in `data`) based on the PCA features contained in `pcadata`. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, if you take $k=3$, perform dimensionality reduction then reconstruction, you should get the original data back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructed data using all the principal components\n",
    "reduced_data=q1.pcadimreduce(data,W,3)\n",
    "reconstructed_data=q1.pcareconstruct(reduced_data,W,3)\n",
    "\n",
    "print (\"This should be small:\",np.max(np.abs(data-reconstructed_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One use of PCA is to help visualize data. The 3-D plots above are a bit hard to read on a 2-D computer screen or when printed out. \n",
    "\n",
    "The following code uses PCA to to reduce the data to $k$ dimensions, and constructs an approximation of the original features using the first $k$ principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your code here\n",
    "reduced_data= q1.pcadimreduce(data,W,2)\n",
    "scatter(reduced_data[:,0],reduced_data[:,1])\n",
    "reconstructed_data= q1.pcareconstruct(reduced_data,W,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the data using two principal components in the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figc = plt.figure()\n",
    "axc = Axes3D(figc)\n",
    "axc.scatter(reconstructed_data[:,0],reconstructed_data[:,1],reconstructed_data[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axc.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)\n",
    "    \n",
    "if enable_interactive:\n",
    "    @interact(elev=(-90, 90), azim=(0, 360))\n",
    "    def view(elev, azim):\n",
    "        axc.view_init(elev, azim)\n",
    "        display(axc.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should see no component of the data along the third principal direction, and the data should lie in a plane. This may be easier to see with the Interactive Mode on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses PCA to reduce the data to one dimension and store the one dimensional PCA feature in `reduced_data_1` and constructs an approximation of the original features using the first  principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your code here\n",
    "reduced_data_1 = q1.pcadimreduce(data,W,1)\n",
    "reconstructed_data_1 = q1.pcareconstruct(reduced_data_1,W,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize this in the original feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figd = plt.figure()\n",
    "axd = Axes3D(figd)\n",
    "axd.scatter(reconstructed_data_1[:,0],reconstructed_data_1[:,1],reconstructed_data_1[:,2],alpha=0.1)\n",
    "c=['r-','g-','y-']\n",
    "for var, pc,color in zip(s, W,c):\n",
    "    axd.plot([0, 2*var*pc[0]], [0, 2*var*pc[1]], [0, 2*var*pc[2]], color, lw=2)\n",
    "    \n",
    "if enable_interactive:\n",
    "    @interact(elev=(-90, 90), azim=(0, 360))\n",
    "    def view(elev, azim):\n",
    "        axd.view_init(elev, azim)\n",
    "        display(axd.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should see no component of the data along the second and third principal direction, and the data should lie along a line. This may be easier with the Interactive Mode on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the PCA feature as a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = hist(reduced_data_1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write a function `pcasvd` to implement PCA via the SVD. <b>(10 points)</b>\n",
    "\n",
    "You will be given as input:\n",
    "- A $(N,d)$ numpy array of data (with each row as a feature vector)\n",
    "\n",
    "Your function should return a tuple consisting of the PCA transformation matrix, and a vector consisting of the amount of variance explained in the data by each PCA feature. Note that the PCA features are ordered in decreasing amount of variance explained.\n",
    "\n",
    "Hints:\n",
    "- The function <a href=\"http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.svd.html\">numpy.linalg.svd</a> will be useful. Use the full SVD (default).\n",
    "- Be careful with how the SVD is returned in `numpy.linalg.svd` (`V` in numpy is the transpose of what is in the notes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your PCA implementation via the SVD is correct (and your Eigendecomposition implementation is correct), principal components should match between the SVD and PCA implementations (up to sign, i.e. the i-th principal component may be the negative of the i-th principal component from the eigendecomposition approach). \n",
    "\n",
    "This is verified by printing out the principal components and the corresponding amount of variance explained. You will not get any credit if the principal components (up to sign) and variances do not match the eigendecomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, run PCA on your data. The PCA transformation is stored in Wsvd, while the amount of variance is stored in ssvd. \n",
    "Wsvd,ssvd=q1.pcasvd(data)\n",
    "\n",
    "# Print out the principal components + the amount of variance they explain\n",
    "for i in range(Wsvd.shape[1]):\n",
    "    print (i+1,\"-th principal component: \", Wsvd[i,:], \"\\t Variance:\",ssvd[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2: PCA for Data Compression (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, you saw an example application of PCA to create eigenfaces. In this part of the lab, we will look at eigenfaces for compression using the <a href=\"http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\">Olivetti faces dataset</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the Olivetti dataset\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "\n",
    "oli = fetch_olivetti_faces()\n",
    "# Height and Width of Images are in h,w. You will need to reshape them to this size display them.\n",
    "h=64\n",
    "w=64\n",
    "X = oli.data\n",
    "\n",
    "X_t=X[20]\n",
    "X=X[:-1]\n",
    "\n",
    "#This centering is unnecessary. it just makes the pictures a bit more readable. \n",
    "\n",
    "X_m=np.mean(X,axis=0)\n",
    "X=X-X_m # center them\n",
    "X_t=X_t-X_m\n",
    "\n",
    "\n",
    "# The data set is in X. You will compress the image X_t. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the Olivetti Faces:\n",
    "<img src=\"olivettifaces.gif\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be making use of Scikit-Learn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">PCA</a> functionality. \n",
    "\n",
    "Three functions will be useful for this problem :\n",
    "- PCA.fit : Finds the requested number of principal components.\n",
    "- PCA.transform : Apply dimensionality reduction (returns the PCA features)\n",
    "- PCA.inverse_transform : Go from PCA features to the original features (Useful for visualizing)\n",
    "\n",
    "You will also find the following useful:\n",
    "- PCA.explained\\_variance\\_ratio\\_ : Percentage of variance explained by each of the principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `unexp_var` that will fit a PCA on X, and return the fraction of **unexplained** variance on `X` by PCA retaining the first $k$ principal components, where  $k=1,\\ldots,200$. You will also return the pca object that has been fit on the image X.\n",
    "\n",
    "The following code uses this function to plot the fraction of unexplained variance. Note that this is a scree plot (normalized by the total variance). \n",
    "\n",
    "`numpy.cumsum` may be useful for this. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = Question2()\n",
    "\n",
    "pca, unexpv = q2.unexp_var(X)\n",
    "\n",
    "plot(np.arange(200)+1,unexpv)\n",
    "xlabel('Number of Principal Components')\n",
    "ylabel('Fraction of unexplained variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code visualizes the first 5 principal components as well as the 30th, 50th and 100th principal components, which are called *eigenfaces* in this context. Our PCA object is called `pca`, and the eigenfaces are contained in `pca.components_`, where each row is a principal component. \n",
    "\n",
    "The following code from Lab 4 may be useful:\n",
    "\n",
    "    figure()\n",
    "    imshow( image , cmap = cm.Greys_r)\n",
    "    \n",
    "where image is the appropriately reshaped principal component (to `h` rows and `w` columns). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=np.asarray([0,1,2,3,4,29,49,99])\n",
    "\n",
    "figure(figsize=(8,16))\n",
    "for i in range(fn.size):\n",
    "    subplot(4,2,i+1)\n",
    "    title(\"{} -th principal component: \".format(fn[i]))\n",
    "    imshow((pca.components_[fn[i]]).reshape((h,w)),cmap=cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later eigenfaces capture more detail as compared to earlier ones (e.g. they're specific to some guy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, you will compress an image, `X_t`, using PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what X_t looks like:\n",
    "imshow((X_t).reshape((h,w)),cmap=cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `pca_approx` that will take as input the `pca` object fit on `X` and returned in the previous part, the input `X_t`, and `i`, and returns an approimation of `X_t` using the the first `i`  principal components (learned from `X`).\n",
    "\n",
    "Do this by the following procedure:\n",
    " \n",
    "1. Transform `X_t` to the PCA features determined by `X`.\n",
    "2. Retain the first `i` PCA features of the transformed `X_t` (set the others to zero). \n",
    "3. Transform the result of step 3 back to the original feature space. \n",
    "\n",
    "Hint : You would want to reshape your `X_t` when using `pca.transform` and `pca.inverse_transform` to (1,-1) since it is a single image.\n",
    "\n",
    "The following code displays the image in `X_t`'s approximation using the first `i` principal components (learned from `X`)  where i=1,10,20,...,100 (i.e. in increments of 10), then 120,140,160,180,200 (i.e. in increments of 20).\n",
    "\n",
    "\n",
    "<b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=np.hstack((np.arange(0,101,10),np.arange(120,201,20)))\n",
    "sizes[0]=1\n",
    "figure(figsize=(8,22))\n",
    "for i in range(sizes.size):\n",
    "    recon_img=q2.pca_approx(X_t, pca, sizes[i])\n",
    "    subplot(8,2,i+1)\n",
    "#     figure()\n",
    "    title(\"{} principal components used: \".format(sizes[i]))\n",
    "    imshow((recon_img).reshape((h,w)),cmap=cm.Greys_r)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: PCA for Classification (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load a data set of digits drawn from zip codes written on US mail. This data set was designed to help get good algorithms to sort mail by zip code automatically. It has been preprocessed a bit, with details given <a href=\"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.info.txt\">here</a>. Each feature vector consists of real values representing grayscale values of a 16 by 16 image of a digit. The training data has 7291 samples, while the validation data has 2007 samples. Note that this is not the same dataset built into scikit- learn -- it is much larger. Use sklearn.decomposition.PCA for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Data\n",
    "#Read in the Training Data\n",
    "traindata_tmp= np.genfromtxt('zip.train', delimiter=' ')\n",
    "#The training labels are stored in \"trainlabels\", training features in \"traindata\"\n",
    "trainlabels=traindata_tmp[:,0]\n",
    "traindata=traindata_tmp[:,1:]\n",
    "#Read in the Validation Data\n",
    "valdata_tmp= np.genfromtxt('zip.val', delimiter=' ')\n",
    "#The validation labels are stored in \"vallabels\", validation features in \"valdata\"\n",
    "vallabels=valdata_tmp[:,0]\n",
    "valdata=valdata_tmp[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 2, you found that the validation error on this data set was 0.056 for 1-NN. \n",
    "\n",
    "Write a function that returns the validation errors using 1-NN on the PCA features using 1,2,...,256 PCA features, the minimum validation error, and number of PCA features used. <b>(15 points)</b>\n",
    "\n",
    "The following code will plot the validation error vs the number of features. While returning the number of features to be used remember that the number of features starts from 1,... and not from 0,... (that is, be careful while using the array indexing). \n",
    "\n",
    "<b> Note that this part will take a lot of time to run depending on your code. Hence, the autograder for your submission will not provide any assertion messages before the deadline (processing each submission would take about 5-7 minutes depending on your code). This function will have 3 outputs, the first will be an np array of size (256, ) , the second will be a float value, and the third will be an int value. Please check this by yourself before submitting your final code, as the autograder will not provide an assertion message for this question before the deadline. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = Question3()\n",
    "\n",
    "ve, min_ve, pca_feat = q3.pca_classify(traindata, trainlabels, valdata, vallabels)\n",
    "    \n",
    "figure()\n",
    "plot(np.arange(256)+1,ve)\n",
    "xlabel('Number of PCA features')\n",
    "ylabel('Validation Error')\n",
    "print (\"Minimum Validation error: \",min_ve)\n",
    "print (\"Number of PCA features to retain:\",pca_feat)\n",
    "axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (For your own understanding): Spectral Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Problem 4 is optional and for your own understanding. It will not be graded by the autograder, and will not count towards your score for this lab.</b>\n",
    "\n",
    "In this problem, you will implement a powerful clustering algorithm known as spectral clustering. It can separate data that in some cases, K-means cannot (as you will see in this problem).\n",
    "\n",
    "Spectral clustering works by forming a graph based on similarities between data vectors, and looking for cluster of data vectors such that the similarity between them is high, but the similarity to vectors outisde the clusters is low (and the clusters aren't too small).\n",
    "\n",
    "See Section 4.3 in the notes for details on how it works, or [this tutorial](https://arxiv.org/abs/0711.0189). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Spectral Clustering Algorithm (Alg. 9):**\n",
    "\n",
    "\n",
    "1. Let $\\tilde{L} = I - D^{-1/2} S D^{-1/2}$ where $D^{-1/2}$ is a square diagonal matrix with $\\frac{1}{\\sqrt{d_i}}$ as the $i$-th entry on the diagonal (where $\\mathbf{d} = S \\mathbf{1}$). \n",
    "2. Take the eigen-decomposition of $\\tilde{L}= U \\Lambda U^\\top$ where $\\Lambda$ is a diagonal matrix containing the eigenvalues of $L$.\n",
    "3. Let $U_K$ be a matrix whose columns are the eigenvectors corresponding to the $K$-smallest eigenvalues of $L$.\n",
    "4. Normalize each row of $U_K$ (i.e. divide each entry on the $i$-th row by the norm of the $i$-th row)\n",
    "5. Apply K-means clustering to the rows of $U_K$ (i.e. treat each row of $U_K$ as a $K$-dimensional feature vector and cluster it). \n",
    "6. Return the cluster labels from step $4$. $\\mathbf{x}_i$ is assigned to the cluster which the $i$-th row of $U_K$ was assigned to. \n",
    "\n",
    "$L$ is known as the normalized Laplacian of the similarity graph, and has many nice properties for analyzing the similarity graph, most of which are beyond the scope of the course. \n",
    "\n",
    "**Note: You don't really need to know why spectral clustering works to do this problem (though it would be nice) -- you just need to be able to implement the algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll make a data set based on the Illinois logo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.nonzero(np.asarray(\n",
    "[[1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,1,1,1,1,1,1,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,0,0,1,1,0,0,0,0,1],\n",
    "[1,0,0,1,1,1,1,1,1,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,0,0,0,0,0,0,0,0,0,0,1],\n",
    "[1,1,1,1,1,1,1,1,1,1,1,1]]))\n",
    "\n",
    "illcmap=ListedColormap(['#131F33','#FA6300'])\n",
    "\n",
    "data=np.c_[tmp[1],tmp[0]]\n",
    "figure()\n",
    "scatter(data[:,0],data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first see what happens if we try to cluster these points using K-means to get 2 clusters. \n",
    "\n",
    "Use `sklearn.cluster.KMeans` to cluster these points into two clusters. \n",
    "\n",
    "Plot the clusters using the colors as the labels you get from K-means clustering as a scatter plot, with `cmap=illcmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf=KMeans(init='k-means++',n_clusters=2)\n",
    "kmf.fit(data)\n",
    "scatter(data[:,0],data[:,1],c=kmf.labels_,cmap=illcmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, you should see something like the right half of the points are in one cluster, and the left half are in the other. The Illinois I should not be separated from the perimeter. In general, K-means cannot produce non-convex clusters (i.e. if you draw a line between any 2 points in a cluster, any point that lies on that line is in that cluster), so it cannot separate the I from the border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement spectral clustering as described above. \n",
    "\n",
    "Recall that `numpy.linalg.eigh` returns the eigenvalues of a matrix in *ascending* order.\n",
    "\n",
    "The code provided already calculates $L$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectralClustering(data,K,C=1):\n",
    "    W=np.exp(-dist.cdist(data,data,'sqeuclidean')/C)\n",
    "    W=W-np.diag(np.diag(W))\n",
    "    Dinv5=np.diag( (W.dot(np.ones(W.shape[0])))**(-0.5) )\n",
    "    L=np.eye(W.shape[0])-Dinv5.dot(W).dot(Dinv5)\n",
    "    # Put your code here\n",
    "    ev,U=np.linalg.eigh(L)\n",
    "    Uk=U[:,:K]\n",
    "    Uk=Uk/((np.sum(Uk**2,axis=1)**0.5)[:, numpy.newaxis])\n",
    "    kmf=KMeans(init='k-means++',n_clusters=K)\n",
    "    kmf.fit(Uk)\n",
    "    return kmf.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run your spectral clustering implementation with 2 clusters on the data in `data`, and plot the data with the colors given by the clusters returned by `spectralClustering` with `cmap=illcmap`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(data[:,0],data[:,1],c=spectralClustering(data,2),cmap=illcmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value of $C=1$ in the spectral clustering code should separate the I from the border (though which will be colored orange and which will be blue will be random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you're doing audio and visual analytics next, I thought I'd leave you with something on that note.\n",
    "\n",
    "You can find a demo of Spectral Clustering applied to image segmentation based on \n",
    "\n",
    "Shi, Jianbo, and Jitendra Malik. \"Normalized cuts and image segmentation.\" IEEE Transactions on pattern analysis and machine intelligence 22.8 (2000): 888-905.\n",
    "\n",
    "at http://scikit-learn.org/stable/auto_examples/cluster/plot_face_segmentation.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# And this concludes the Machine Learning section of the course! Good luck with your future endeavors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
